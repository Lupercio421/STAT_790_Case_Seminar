---
title: "Spark and timemodels introduction"
author: Daniel Lupercio
output: html_document
---

```{r}
library(sparklyr)
library(tidymodels)
library(modeltime)
library(tidyverse)
library(timetk)
```

To run Spark locally: 

```{r}
#sparklyr::spark_install()
# Sys.setenv(JAVA_HOME=" ")
# Sys.setenv(SPARK_PREPEND_CLASSES = "")
sc <- spark_connect(master = "local")
# spark_connect(method = "databricks")
```

Next, we register the Spark Backend using parallel_start(sc, .method = "spark"). This is a helper to set up the registerDoSpark() foreach adaptor. In layman’s terms, this just means that we can now run parallel using Spark.

```{r}
parallel_start(sc, .method = "spark")
```

### Data Preperation (for Nested Forecasting)

The dataset we’ll be forecasting is the walmart_sales_weekly, which we modify to just include 3 columns: “id”, “date”, “value”.

- The id feature is the grouping variable.
- The date feature contains timestamps.
- The value feature is the sales value for the Walmart store-department combination.

```{r}
walmart_sales_weekly %>%
    select(id, Date, Weekly_Sales) %>%
    set_names(c("id", "date", "value")) %>%
    group_by(id) %>% #returns single rows of id
    plot_time_series(date, value, .facet_ncol = 2, .interactive = F)
```

We prepare as nested data using the Nested Forecasting preparation functions.

- extend_timeseries(): This extends each one of our time series into the future by 52 timestamps (this is one year for our weekly data set).

- nest_timeseries(): This converts our data to the nested data format indicating that our future data will be the last 52 timestamps (that we just extended).

- split_nested_timeseries(): This adds indicies for the train / test splitting so we can develop accuracy metrics and determine which model to use for which time series.

```{r}
nested_data_tbl <- walmart_sales_weekly %>%
    select(id, Date, Weekly_Sales) %>%
    set_names(c("id", "date", "value")) %>%
    extend_timeseries(
        .id_var        = id,
        .date_var      = date,
        .length_future = 52
    ) %>%
    nest_timeseries(
        .id_var        = id,
        .length_future = 52
    ) %>%
    
    split_nested_timeseries(
        .length_test = 52
    )
print(nested_data_tbl)
```

You’ll notice our data frame (tibble) is only 7 rows and 4 columns. This is because we’ve nested our data. If you examine each of the rows, you’ll notice each row is an ID. And we have “tibbles” in each of the other columns.

That is nested data!

### Modeling

We’ll create two unfitted models: XGBoost and Prophet. Then we’ll use modeltime_nested_fit() to iteratively fit the models to each of the time series using the Spark Backend.


#### Model 1: XGBoost

We create the XGBoost model on features derived from the date column. This gets a bit complicated because we are adding recipes to process the data. Basically, we are creating a bunch of features from the date column in each of the time series.

- First, we use extract_nested_train_split(nested_data_tbl, 1) to extract the first time series, so we can begin to create a “recipe”

- Once we develop a recipe, we add “steps” that build features. We start by creating timeseries signature features from the date column. Then we remove “date” and further process the signature features.

- Then we create an XGBoost model by developing a tidymodels “workflow”. The workflow combines a model (boost_tree() in this case) with a recipe that we previously created

- The output is an unfitted workflow that will be applied to all 7 of our timeseries.

```{r}
#recipe for XGBoost
rec_xgb <- recipe(value ~ ., extract_nested_train_split(nested_data_tbl, 1)) %>%
    step_timeseries_signature(date) %>%
    step_rm(date) %>%
    step_zv(all_predictors()) %>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)

#workflow for XGBoost
wflw_xgb <- workflow() %>%
    add_model(boost_tree("regression") %>% set_engine("xgboost")) %>%
    add_recipe(rec_xgb)

wflw_xgb
```

### Prophet

Next, we create a prophet workflow. The process is actually simpler than XGBoost because Prophet doesn’t require all of the preprocessing recipe steps that XGBoost does.

```{r}
rec_prophet <- recipe(value ~ date, extract_nested_train_split(nested_data_tbl))

wflw_prophet <- workflow() %>% 
  add_model(prophet_reg("regression", seasonality_yearly = TRUE) %>% 
              set_engine("prophet")) %>% 
  add_recipe(rec_prophet)

print(wflw_prophet)

```

### Nested Forecasting with Spark

Now, the beauty is that everything is set up for us to perform the nested forecasting with Spark. We simply use modeltime_nested_fit() and make sure it uses the Spark Backend by setting control_nested_fit(allow_par = TRUE).

Note that this will take about 20-seconds because we have a one-time cost to move data, libraries, and environment variables to the Spark clusters. But the good news is that when we scale up to 10,000+ time series, that the one-time cost is minimal compared to the speed up from distributed computation.

```{r}
nested_modeltime_tbl <- nested_data_tbl %>%
    modeltime_nested_fit(
        wflw_xgb,
        wflw_prophet,
        control = control_nested_fit(allow_par = TRUE, verbose = TRUE)
    )
```

The nested modeltime object has now fit the models using Spark. You’ll see a new column added to our nested data with the name “.modeltime_tables”. This contains 2 fitted models (one XGBoost and one Prophet) for each time series.

```{r}
nested_modeltime_tbl
```

### Model Test Accuracy

We can observe the results. First, we can check the accuracy for each model.

- Let’s use extract_nested_test_accuracy() to extract the logged accuracy table.

- We can format it as an HTML table with table_modeltime_accuracy(). This function is great for reports!

```{r}
nested_modeltime_tbl %>%
  extract_nested_test_accuracy() %>%
  table_modeltime_accuracy(.interactive = F)
```

### Test Forecast

Next, we can examine the test forecast for each of the models.

- We can use extract_nested_test_forecast() to extract the logged forecast and visualize how each did.

- We group_by(id) then pipe (%>%) into plot_modeltime_forecast() to make the visualization (great for reports and shiny apps!)

```{r}
nested_modeltime_tbl %>%
  extract_nested_test_forecast() %>%
  group_by(id) %>%
  plot_modeltime_forecast(.facet_ncol = 2, .interactive = F)
```

### Close Clusters and Shutdown Spark

```{r}
# Unregisters the Spark Backend
parallel_stop()

# Disconnects Spark
spark_disconnect_all()
```

# STAT 790 Attempt

```{r}
bx_ts %>% plot_time_series(month, total_waste)
```

```{r}
bx_model_fit_arima_no_boost <- arima_reg() %>% 
  set_engine(engine = "auto_arima") %>% 
  fit(total_waste ~ month, data = bx_ts)

```

